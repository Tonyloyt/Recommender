{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps can be found [here]() or check the [Collaborative Filtering note]() for the detailed explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../'\n",
    "# Load the data\n",
    "with open(dir + 'data/user_to_movie.json', 'rb') as f:\n",
    "    user_to_movie = pickle.load(f)\n",
    "\n",
    "with open(dir + 'data/movie_to_user.json', 'rb') as f:\n",
    "    movie_to_user = pickle.load(f)\n",
    "\n",
    "with open(dir + 'data/um_to_rating_tr.json', 'rb') as f:\n",
    "    um_to_rating_tr = pickle.load(f)\n",
    "\n",
    "with open(dir + 'data/um_to_rating_te.json', 'rb') as f:\n",
    "    um_to_rating_te = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the data: 1000 users & 200 movies\n"
     ]
    }
   ],
   "source": [
    "# Count the number of users\n",
    "N = np.max(list(user_to_movie.keys())) + 1                               # User id starts from 0\n",
    "\n",
    "# Count the number of movies\n",
    "m_tr = np.max(list(movie_to_user.keys()))\n",
    "\n",
    "# Get the maximum movie id from the test set for the movies not in the train set\n",
    "te_movie_list = [m for (u, m), r in um_to_rating_te.items()]\n",
    "m_te = np.max(te_movie_list)\n",
    "\n",
    "M = max(m_tr, m_te) + 1\n",
    "print(\"The size of the data: {} users & {} movies\".format(N, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrix Factorization from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms need to be calculated.\n",
    "- ### Loss fuction J = $\\sum_{a, b} (r - \\hat{r})^2$ + $||\\lambda||$<sub>2</sub>$^2$ \n",
    "- ### $\\hat{r}$<sub>a, m</sub> = W<sub>a</sub> * U<sub>m</sub> + b<sub>a</sub> + c<sub>m</sub> + $\\mu$\n",
    "\n",
    "where ***b<sub>a</sub>***, ***c<sub>m</sub>*** are the bias terms and ***$\\mu$*** is the global average for each user ***a***, movie ***b***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the get_loss funtion\n",
    "def get_loss(x):\n",
    "    '''\n",
    "    Calculating the loss\n",
    "    input: the data in the form of \"(user, movie) : rating\"\n",
    "    output: the mean_squared_error\n",
    "    '''\n",
    "    n = len(x)\n",
    "    loss = 0\n",
    "    for (a, m), r in x.items():\n",
    "        pred = W[a].dot(U[m]) + b[a] + c[m] + mu\n",
    "        loss += (r - pred)**2\n",
    "    return loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters to update\n",
    "k = 10                              # latent features\n",
    "\n",
    "W = np.random.randn(N, k)\n",
    "U = np.random.randn(M, k)\n",
    "\n",
    "b = np.zeros(N)\n",
    "c = np.zeros(M)\n",
    "\n",
    "mu = np.mean(list(um_to_rating_te.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the W and U terms are dependent to each other, ***alternative least square (ALS)*** technique will be applied to get the gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch train loss : 3.0192114400891876, test lss : 3.7774942620489265\n",
      "1th epoch train loss : 2.9784769462496645, test lss : 3.7381509442094645\n",
      "2th epoch train loss : 3.0100592132613815, test lss : 3.7719534264977774\n",
      "3th epoch train loss : 3.0089423500456713, test lss : 3.7708839408601\n",
      "4th epoch train loss : 3.00987620889248, test lss : 3.771876601125883\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 5\n",
    "reg = 20                # regularization penalty\n",
    "\n",
    "tr_losses = []\n",
    "te_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Gradient descent on W and b (the parameters related to the user a)\n",
    "    for a in range(N):\n",
    "\n",
    "        # Initialize variables for calculation\n",
    "        term_1 = np.eye(k)*reg\n",
    "        term_2 = np.zeros(k)\n",
    "        b_a = 0\n",
    "\n",
    "        # Updating the parameters along the movie ids while the user id is constant\n",
    "        for m in user_to_movie[a]:\n",
    "            r_a_m = um_to_rating_tr[(a, m)]\n",
    "\n",
    "            term_1 += np.dot(U[m], U[m].T)\n",
    "            term_2 += (r_a_m - b[a] - c[m] - mu)*U[m]\n",
    "            b_a += r_a_m - U[m].dot(W[a]) - c[m] - mu\n",
    "\n",
    "        W[a] = np.linalg.solve(term_1, term_2)\n",
    "        b[a] = 1/(len(user_to_movie[a])*(1 + reg)) * b_a\n",
    "\n",
    "\n",
    "    # Gradient descent on U and c (the parameters related to the movie m)\n",
    "    for m in range(M):\n",
    "\n",
    "        # Initialize variables for calculation\n",
    "        term_1 = np.eye(k)*reg\n",
    "        term_2 = np.zeros(k)\n",
    "        c_m = 0\n",
    "\n",
    "        # Updating the parameters along the user ids while the movie id is constant\n",
    "        try:\n",
    "            for a in movie_to_user[m]:\n",
    "                r_a_m = um_to_rating_tr[(a, m)]\n",
    "\n",
    "                term_1 += np.dot(W[a], W[a].T)\n",
    "                term_2 += (r_a_m - b[a] - c[m] - mu)*W[a]\n",
    "                c_m += r_a_m - W[a].dot(U[m]) - b[a] - mu\n",
    "\n",
    "            U[b] = np.linalg.solve(term_1, term_2)\n",
    "            c[m] = 1/(len(movie_to_user[m])*(1+reg)) * c_m\n",
    "\n",
    "        # for the case the movie m has no rating\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    tr_loss = get_loss(um_to_rating_tr)\n",
    "    te_loss = get_loss(um_to_rating_te)\n",
    "    print(\"{}th epoch train loss : {}, test lss : {}\".format(epoch, tr_loss, te_loss))\n",
    "\n",
    "    tr_losses.append(tr_loss)\n",
    "    te_losses.append(te_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train loss:  [3.0192114400891876, 2.9784769462496645, 3.0100592132613815, 3.0089423500456713, 3.00987620889248]\n",
      "Total test loss:  [3.7774942620489265, 3.7381509442094645, 3.7719534264977774, 3.7708839408601, 3.771876601125883]\n"
     ]
    }
   ],
   "source": [
    "print(\"total train loss: \", tr_losses)\n",
    "print(\"Total test loss: \", te_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evaluation\n",
    "plt.plot(tr_losses, label = \"train loss\")\n",
    "plt.plot(te_losses, label = \"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A little bit faster way.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looping the user id and movie id repeatedly for each time, we can convert the data into an array so that the computation can speed up. This is using the real power of numpy array. \n",
    "\n",
    "Firstly, convert the `user_to_movie` dictionary to contain the rating data in the form of \"**user_id : ( the list of movie_ids : an array of rating)**\". We'll call this as `user_to_mr`. Also, convert the `movie_to_user` dictionary as a \"**movie_id : ( the list of user_ids : an array of rating)**\", which will be `movie_to_ur_tr`. The same process will be applied and create `movie_to_ur_te` as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Converting the dictionaries to include the rating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redesigning the user_to_movie and movie_to_user dictionary to contain the rating data\n",
    "# Create the dictionary in the form of \"user_id : (movie_ids, rating_arr)\"\n",
    "user_to_mr = {}\n",
    "for a, movies in user_to_movie.items():\n",
    "    rating_list = [um_to_rating_tr[(a, m)] for m in movies]\n",
    "    rating_arr = np.array(rating_list)\n",
    "    user_to_mr[a] = (movies, rating_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([49, 45, 147, 3, 64, 31, 106, 33, 47, 170, 124, 0, 130, 159, 151, 110, 125, 113, 162, 140, 107, 169, 20, 12, 83, 37, 74, 69, 198, 164, 136, 72, 109, 149, 139, 154, 161, 19, 85, 9, 183, 97, 57, 126, 89, 173, 196, 44, 119, 43, 108, 186, 46, 28, 1, 187, 53, 7, 78, 90, 115, 54, 25, 60, 171, 102, 133, 146, 55, 192, 160, 81, 112, 91, 39, 58, 70, 75, 138, 99, 26, 181, 41, 103, 194, 123, 96, 61, 2, 199, 144, 114, 30, 150, 118, 73, 48, 134, 122, 176, 36, 17, 190, 56, 88, 50, 68, 172, 66, 6, 180, 14, 10, 152, 23, 178, 4, 65, 76, 77, 104, 40, 158, 82, 132, 62, 15, 193, 22, 94, 84, 142, 67, 105, 95, 71, 117, 51, 93, 177, 185, 135, 35, 195, 191, 167, 197, 153, 16, 18, 131, 13, 128, 38, 189, 86, 21, 52, 175, 32, 163, 79], array([2.5, 3.5, 4. , 5. , 3. , 4. , 4. , 4.5, 5. , 2.5, 4. , 5. , 4. ,\n",
      "       3.5, 5. , 3.5, 3. , 5. , 4. , 4. , 4. , 3. , 4. , 4. , 5. , 4.5,\n",
      "       4.5, 3.5, 4. , 4. , 4.5, 4. , 3. , 4.5, 3.5, 4. , 3.5, 4. , 4.5,\n",
      "       5. , 4. , 4. , 3. , 5. , 4. , 4. , 5. , 5. , 5. , 5. , 4. , 4. ,\n",
      "       2. , 5. , 4.5, 4. , 4.5, 5. , 5. , 5. , 4. , 4.5, 4. , 5. , 3. ,\n",
      "       4. , 4. , 3.5, 3. , 5. , 4. , 3.5, 5. , 5. , 3.5, 3.5, 3.5, 4. ,\n",
      "       3.5, 5. , 4.5, 3.5, 3.5, 4. , 3. , 4.5, 4. , 4.5, 4.5, 4.5, 4.5,\n",
      "       4.5, 4. , 4. , 3.5, 2. , 5. , 4.5, 5. , 4. , 3. , 5. , 4. , 5. ,\n",
      "       3.5, 5. , 2.5, 4. , 4. , 4. , 4. , 4.5, 5. , 3.5, 4.5, 4. , 4. ,\n",
      "       3. , 5. , 3. , 4. , 4.5, 3. , 4.5, 4.5, 5. , 4. , 4. , 5. , 3. ,\n",
      "       2.5, 3. , 4.5, 3.5, 2. , 4. , 3.5, 5. , 5. , 3. , 2.5, 4. , 5. ,\n",
      "       3.5, 5. , 3.5, 3.5, 4.5, 4.5, 4.5, 4. , 3. , 4. , 3. , 2.5, 2.5,\n",
      "       5. , 4. , 5. , 3. , 3. , 4.5]))\n"
     ]
    }
   ],
   "source": [
    "# Check the result \n",
    "print(user_to_mr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary in the form of \" movie_id : (user_ids, rating_arr)\"\n",
    "movie_to_ur_tr = {}\n",
    "for m, users in movie_to_user.items():\n",
    "    rating_list = [um_to_rating_tr[(a, m)] for a in users]\n",
    "    rating_arr = np.array(rating_list)\n",
    "    movie_to_ur_tr[m] = (users, rating_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([625, 822, 117, 314, 286, 635, 214, 71, 123, 295, 821, 716, 816, 25, 29, 490, 513, 315, 773, 769, 444, 634, 211, 474, 328, 770, 169, 767, 500, 283, 174, 935, 44, 237, 807, 418, 729, 9, 116, 621, 195, 981, 554, 393, 335, 238, 179, 387, 172, 370, 762, 66, 125, 873, 511, 330, 2, 431, 612, 530, 139, 533, 733, 988, 557, 323, 696, 175, 825, 309, 600, 408, 373, 944, 524, 28, 415, 363, 813, 388, 900, 426, 744, 699, 517, 203, 667, 590, 23, 796, 134, 423, 686, 544, 862, 893, 445, 571, 496, 845, 180, 980, 383, 692, 221, 279, 660, 141, 151, 263, 148, 347, 497, 546, 947, 681, 854, 652, 407, 952, 859, 661, 36, 792, 826, 235, 433, 52, 814, 212, 39, 292, 76, 632, 897, 563, 459, 569, 784, 983, 748, 386, 112, 793, 532, 467, 808, 568, 582, 84, 35, 199, 62, 394, 819, 26, 88, 468, 503, 877, 704, 94, 891, 954, 669, 478, 342, 523, 789, 802, 202, 598, 772, 937, 127, 434, 982, 325, 375, 648, 603, 114, 756, 646, 797, 162, 781, 800, 217, 709, 633, 857, 640, 615, 189, 167, 832, 187, 972, 92, 200, 82, 924, 771, 708, 190, 13, 775, 970, 495, 4, 356, 853, 927, 135, 882, 798, 96, 665, 765, 671, 262, 327, 143, 504, 60, 22, 785, 997, 920, 653, 359, 479, 293, 996, 391, 493, 341, 177, 941, 166, 737, 604, 973, 561, 928, 321, 649, 753, 385, 16, 154, 440, 147, 916, 871, 918, 543, 432, 761, 477, 170, 899, 269, 222, 556, 666, 574, 204, 404, 246, 191, 515, 874, 209, 623, 1, 560, 197, 7, 930, 714, 967, 210, 86, 791, 599, 153, 834, 296, 779, 488, 74, 268, 346, 57, 663, 245, 690, 965, 453, 866, 626, 829, 651, 752, 624, 192, 976, 917, 89, 258, 369, 149, 759, 40, 727, 12, 637, 366, 70, 953, 713, 990, 395, 514, 763, 442, 78, 656, 619, 219, 338, 923, 674, 208, 710, 14, 507, 672, 38, 522, 382, 193, 487, 90, 378, 413, 650, 566, 406, 118, 106, 282, 818, 194, 964, 124, 8, 242, 950, 565, 365, 817, 894, 481, 687, 261, 913, 643, 589, 578, 77, 875, 301, 374, 81, 358, 912, 343, 782, 828, 215, 298, 724, 629, 290, 925, 176, 21, 518, 835, 608, 688, 823, 55, 631, 392, 271, 872, 628, 577, 673, 929, 528, 412, 11, 259, 895, 718, 940, 289, 138, 241, 379, 465, 79, 198, 472, 384, 805, 307, 213, 287, 6, 572, 284, 595, 706, 450, 173, 536, 430, 693, 312, 100, 216, 682, 933, 801, 5, 157, 776, 841, 126, 101, 348, 99, 402, 685, 337, 639, 836, 110, 584, 115, 509, 34, 266, 777, 887, 398, 304, 986, 616, 168, 884, 742, 678, 644, 492, 340, 992, 422, 264, 747, 303, 502, 436, 273, 553, 949, 417, 915, 233, 906, 460, 910, 145, 75, 466, 429, 878, 480, 668, 302, 831, 607, 411, 860, 454, 824, 390, 783, 957, 470, 322, 732, 984, 401, 613, 3, 879, 344, 98, 32, 564, 171, 320, 788, 815, 587, 51, 455, 539, 934, 254, 251, 592, 820, 966, 811, 142, 591, 107, 227, 427, 310, 265, 400, 109, 281, 122, 437, 730, 224, 739, 922, 766, 486, 856, 226, 969, 499, 987, 597, 734, 701, 236, 746, 880, 764, 867, 555, 691, 562, 182, 645, 993, 186, 700, 317, 712, 501, 368, 641, 583, 188, 181, 313, 120, 868, 757, 889, 526, 617, 447, 324, 794, 506, 435, 525, 372, 647, 164, 683, 559, 247, 955, 694, 223, 152, 943, 30, 994, 659, 234, 438, 677, 288, 720, 605, 622, 547, 956, 715, 249, 244, 155, 158, 848, 998, 827, 319, 946, 239, 58, 397, 549, 61, 91, 146, 136, 83, 698, 111, 331, 810, 178, 705, 938, 575, 421, 414, 620, 95, 93, 367, 786, 721, 750, 102, 461, 951, 840, 354, 942, 896, 731, 159, 977, 483, 921, 609, 908, 336, 243, 905, 855, 351, 252, 876, 864, 593, 291, 863, 697, 425, 723, 43, 795, 535, 534, 339, 428, 416, 87, 482, 381, 248, 919, 806, 54, 441, 443, 297, 357, 463, 161, 103, 883, 838, 885, 257, 540, 305, 350, 229, 542, 804, 278, 865, 132, 50, 452, 371, 48, 20, 205, 774, 225, 971, 308, 46, 160, 376, 703, 73, 475, 586, 658, 59, 131, 861, 396, 448, 360, 449, 758, 150, 931, 707, 277, 56, 484, 299, 735, 790, 318, 676, 833, 230, 424, 531, 978, 537, 844, 726, 260, 610, 489, 255, 49, 491, 476, 24, 803, 274, 285, 377, 958, 755, 144, 740, 602], array([5. , 4. , 3.5, 4. , 4. , 5. , 5. , 3.5, 3. , 4.5, 3.5, 4.5, 3.5,\n",
      "       4. , 4.5, 4. , 4. , 4. , 5. , 4.5, 4. , 5. , 4. , 3.5, 5. , 2. ,\n",
      "       3.5, 2.5, 5. , 3. , 4. , 4.5, 4.5, 4. , 4. , 3.5, 4.5, 4. , 2. ,\n",
      "       3.5, 4. , 4. , 4. , 4. , 4. , 4.5, 5. , 4. , 5. , 5. , 5. , 5. ,\n",
      "       1.5, 3. , 5. , 4. , 4. , 4. , 5. , 4. , 3. , 3.5, 4. , 5. , 4. ,\n",
      "       4. , 3. , 4. , 5. , 4.5, 4. , 4. , 4. , 3.5, 5. , 0.5, 4. , 3. ,\n",
      "       3. , 4. , 3. , 4. , 4.5, 3. , 2.5, 4. , 3. , 5. , 5. , 4. , 2.5,\n",
      "       4. , 5. , 3. , 3. , 4. , 4.5, 1. , 3.5, 2. , 5. , 3. , 4. , 4. ,\n",
      "       5. , 5. , 2. , 4. , 4. , 1. , 0.5, 4.5, 4. , 3. , 2. , 0.5, 5. ,\n",
      "       4.5, 3.5, 5. , 4.5, 3.5, 2. , 5. , 3. , 3. , 4.5, 2.5, 4.5, 5. ,\n",
      "       5. , 5. , 3. , 3. , 1. , 4.5, 5. , 5. , 3.5, 3.5, 4. , 3. , 5. ,\n",
      "       4. , 1.5, 1.5, 5. , 5. , 3. , 3. , 4. , 4. , 5. , 4. , 2. , 4.5,\n",
      "       4. , 4. , 1. , 5. , 5. , 1. , 2. , 4. , 4. , 3. , 4. , 2. , 4. ,\n",
      "       2.5, 4. , 3.5, 4.5, 3.5, 5. , 5. , 2. , 2. , 5. , 4. , 4.5, 4. ,\n",
      "       5. , 5. , 2. , 4. , 1. , 4. , 5. , 5. , 4. , 4. , 3. , 4. , 5. ,\n",
      "       2.5, 3.5, 4. , 4. , 4.5, 5. , 4. , 3. , 5. , 4.5, 5. , 4. , 3. ,\n",
      "       4. , 3.5, 5. , 4.5, 3. , 4. , 4. , 5. , 3. , 3.5, 4. , 5. , 3. ,\n",
      "       2.5, 4. , 4. , 0.5, 5. , 3. , 5. , 5. , 5. , 4. , 5. , 4. , 5. ,\n",
      "       4. , 0.5, 3. , 4. , 4. , 5. , 3.5, 4. , 2.5, 4. , 1. , 3.5, 3.5,\n",
      "       4. , 4. , 3.5, 4. , 3. , 4. , 4. , 4.5, 3. , 3. , 5. , 5. , 3. ,\n",
      "       4.5, 3. , 3. , 4.5, 5. , 5. , 3. , 4. , 3.5, 3. , 3.5, 1. , 4.5,\n",
      "       3.5, 3.5, 4. , 4.5, 5. , 4. , 3. , 2.5, 2.5, 3.5, 2.5, 5. , 4. ,\n",
      "       1. , 5. , 4. , 4.5, 4.5, 3. , 2. , 4. , 5. , 4. , 1.5, 5. , 3.5,\n",
      "       5. , 4.5, 4. , 4.5, 4. , 4. , 2. , 4. , 3. , 4. , 4. , 2. , 4.5,\n",
      "       4. , 5. , 2. , 3. , 3.5, 3.5, 4. , 1. , 2.5, 3.5, 3.5, 4. , 2. ,\n",
      "       5. , 4.5, 4.5, 4.5, 5. , 3. , 4. , 3. , 3. , 4. , 4. , 4. , 4. ,\n",
      "       5. , 4. , 4. , 4. , 2.5, 4. , 2.5, 2.5, 4. , 5. , 4. , 3. , 4.5,\n",
      "       4.5, 5. , 5. , 3.5, 2. , 3.5, 4.5, 3. , 5. , 1.5, 2. , 5. , 5. ,\n",
      "       1.5, 3. , 2. , 4.5, 3.5, 3. , 4.5, 1. , 3.5, 3. , 4. , 3.5, 5. ,\n",
      "       4. , 4. , 4. , 4. , 5. , 4.5, 4. , 4. , 3.5, 3.5, 4. , 5. , 5. ,\n",
      "       3. , 5. , 3. , 4. , 4. , 2. , 5. , 4. , 5. , 5. , 5. , 3. , 2. ,\n",
      "       4. , 4.5, 4.5, 4. , 3. , 3. , 5. , 3. , 5. , 4. , 4.5, 5. , 3. ,\n",
      "       5. , 5. , 4. , 3. , 3. , 3. , 2. , 4.5, 3.5, 4. , 4. , 3.5, 3. ,\n",
      "       3.5, 3.5, 5. , 4. , 4.5, 2.5, 4. , 3. , 4. , 4. , 2.5, 2. , 4.5,\n",
      "       5. , 2.5, 4. , 4.5, 4. , 4. , 4. , 5. , 3. , 5. , 4.5, 5. , 3. ,\n",
      "       5. , 3. , 3. , 3. , 2.5, 5. , 4. , 3. , 2. , 4. , 5. , 4.5, 4. ,\n",
      "       4. , 4. , 5. , 2. , 5. , 3.5, 4. , 5. , 5. , 5. , 3.5, 4.5, 3. ,\n",
      "       4. , 2.5, 5. , 4. , 4. , 3. , 3.5, 3.5, 4. , 5. , 4. , 5. , 4. ,\n",
      "       4. , 5. , 4. , 5. , 3.5, 5. , 4. , 3. , 5. , 5. , 5. , 4. , 5. ,\n",
      "       3. , 4.5, 5. , 4. , 4. , 5. , 4. , 4.5, 4. , 5. , 3. , 4.5, 3. ,\n",
      "       2. , 4.5, 3. , 4. , 5. , 3.5, 4.5, 4. , 2. , 5. , 5. , 4. , 5. ,\n",
      "       5. , 4. , 5. , 4. , 4. , 3. , 0.5, 5. , 5. , 1. , 5. , 5. , 3. ,\n",
      "       4.5, 5. , 5. , 3. , 3.5, 3.5, 2.5, 4. , 3. , 4.5, 3.5, 4. , 2. ,\n",
      "       2. , 4. , 5. , 4. , 5. , 4. , 5. , 4.5, 4.5, 3.5, 1. , 2. , 3. ,\n",
      "       5. , 4.5, 4.5, 2. , 5. , 2.5, 4. , 3. , 3. , 3. , 4. , 4. , 1. ,\n",
      "       3.5, 3.5, 4.5, 3.5, 3.5, 5. , 2. , 4. , 4.5, 1.5, 4. , 4. , 4. ,\n",
      "       4.5, 4. , 4.5, 4. , 4. , 2.5, 4. , 3.5, 5. , 3. , 4. , 4. , 5. ,\n",
      "       5. , 2.5, 3. , 3.5, 3. , 4.5, 5. , 4. , 4.5, 3. , 3.5, 2. , 5. ,\n",
      "       4. , 4.5, 5. , 4.5, 3. , 2. , 5. , 4.5, 5. , 5. , 4. , 4. , 4.5,\n",
      "       1. , 3.5, 4. , 4.5, 5. , 5. , 3. , 3.5, 4. , 4. , 4. , 3. , 4. ,\n",
      "       4. , 4. , 4. , 5. , 3. , 4.5, 2. , 5. , 4. , 2. , 4.5, 4. , 3. ,\n",
      "       4. , 5. , 1.5, 2.5, 4. , 4. , 4. , 5. , 4. , 4. , 3.5, 4. , 4. ,\n",
      "       5. , 4. , 5. , 3. , 3.5, 4.5, 5. , 3. , 4.5, 2. , 5. , 4. , 4. ,\n",
      "       5. , 1.5, 4.5, 3. , 4.5, 2.5, 3.5, 5. , 5. , 3. , 4. , 4. , 4.5,\n",
      "       5. , 5. , 3. , 4.5, 4.5, 3.5, 5. , 3. , 5. , 2.5, 4.5, 2. , 3. ,\n",
      "       5. , 4. , 4.5, 0.5, 3.5, 3. , 4.5, 4. , 4. , 4. , 2. , 2.5, 3. ,\n",
      "       3.5, 3.5, 4. , 4.5, 3. , 3.5, 2. , 4. , 5. , 0.5, 5. , 4. , 3.5,\n",
      "       5. , 4. , 5. , 5. , 4.5, 4. , 4. , 4. , 3. , 5. , 2. , 3. , 4. ,\n",
      "       5. , 3.5, 4. , 3. , 0.5, 2.5, 4. , 4. , 2.5, 4. , 3.5, 4.5, 4. ,\n",
      "       3. , 4. , 5. , 4. , 4.5, 5. , 5. , 5. ]))\n"
     ]
    }
   ],
   "source": [
    "# Check the result \n",
    "print(movie_to_ur_tr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the form of the outcome is the user ids in a list and the ratings in an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same process with the test set\n",
    "movie_to_ur_te = {}\n",
    "for (a, m), r in um_to_rating_te.items():\n",
    "    # If a movie appear for the first time, save the user and rating data as a list\n",
    "    if m not in movie_to_ur_te:\n",
    "        movie_to_ur_te[m] = [[a], [r]]\n",
    "    # If a movie already exist, add the new user id and rating data to the corresponding index\n",
    "    # Note that the final values will be \"movie id : ( a list of user ids , a list of ratings )\"\n",
    "    else:\n",
    "        movie_to_ur_te[m][0].append(a)\n",
    "        movie_to_ur_te[m][1].append(r)\n",
    "\n",
    "# Convert the list of ratings as an array for computation speed later on\n",
    "for m, (users, ratings) in movie_to_ur_te.items():\n",
    "    movie_to_ur_te[m][1] = np.array(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[642, 614, 140, 736, 545, 960, 959, 684, 326, 250, 256, 948, 232, 458, 675, 570, 780, 850, 389, 0, 15, 97, 521, 353, 355, 869, 272, 362, 529, 184, 276, 751, 914, 664, 228, 352, 909, 345, 68, 636, 104, 399, 903, 129, 294, 926, 787, 689, 446, 585, 657, 275, 743, 510, 594, 108, 680, 65, 738, 842, 183, 156, 137, 576, 439, 185, 902, 573, 512, 206, 932, 419, 858, 778, 207, 334, 901, 218, 904, 630, 121, 105, 638, 494, 72, 113, 329, 364, 240, 37, 580, 627, 47, 843, 945, 849, 519, 548, 457, 27, 485, 306, 852, 498, 270, 41, 702, 963, 907, 754, 33, 420, 847, 991, 999, 596, 31, 567, 300, 749, 892, 333, 473, 812, 768, 403, 985, 552, 601, 67, 253, 551, 962, 18, 508, 469, 717, 890, 830, 745, 349, 231, 881, 837, 611, 799, 579, 968, 870, 975, 133, 888, 538, 17, 119, 809, 332, 679, 606, 898, 961, 464, 558, 655, 719, 979, 80, 361, 741, 722, 196, 911, 316, 851, 936, 670, 69, 280, 85, 42, 64, 989, 527, 505, 220, 839, 711, 846, 728, 939, 409, 163, 654, 618], array([2.5, 4.5, 2. , 4. , 5. , 5. , 4.5, 0.5, 3.5, 5. , 4. , 4. , 3. ,\n",
      "       3. , 1.5, 3.5, 4. , 4.5, 3.5, 4. , 4.5, 5. , 2. , 2.5, 3.5, 4. ,\n",
      "       5. , 2.5, 5. , 2.5, 5. , 3.5, 3.5, 5. , 5. , 3.5, 4. , 5. , 2.5,\n",
      "       4. , 2. , 3.5, 2. , 2.5, 4.5, 1. , 4. , 3. , 2.5, 5. , 5. , 5. ,\n",
      "       5. , 3.5, 4. , 5. , 3.5, 3.5, 3. , 5. , 4.5, 4. , 2. , 5. , 4. ,\n",
      "       4.5, 4.5, 2.5, 5. , 4. , 4. , 5. , 3. , 4. , 5. , 3. , 2. , 4. ,\n",
      "       5. , 5. , 3. , 4. , 5. , 3.5, 3. , 3.5, 4. , 5. , 5. , 4.5, 3. ,\n",
      "       3.5, 2.5, 3.5, 5. , 5. , 3. , 4. , 5. , 3. , 4. , 4.5, 5. , 5. ,\n",
      "       5. , 4. , 3. , 3. , 2. , 4. , 3.5, 4. , 3. , 3. , 4. , 5. , 5. ,\n",
      "       4.5, 3. , 4. , 4. , 4.5, 4.5, 4. , 5. , 3.5, 3. , 5. , 4. , 2.5,\n",
      "       4. , 3. , 5. , 5. , 3.5, 4. , 3. , 4.5, 3. , 5. , 3.5, 4. , 4.5,\n",
      "       4. , 4. , 4. , 4. , 5. , 4. , 3. , 4. , 1. , 1.5, 5. , 5. , 4. ,\n",
      "       4. , 5. , 4. , 4. , 3. , 3.5, 4.5, 4.5, 4.5, 4. , 4. , 5. , 4. ,\n",
      "       4. , 1. , 4. , 3.5, 5. , 5. , 4. , 4.5, 4. , 3.5, 5. , 5. , 4. ,\n",
      "       3.5, 4. , 4. , 2. , 5. , 3.5, 4. , 0.5, 3. , 4. , 2.5, 1. ])]\n"
     ]
    }
   ],
   "source": [
    "# Check the result \n",
    "print(movie_to_ur_te[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Matrix factorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the get_loss funtion\n",
    "def get_loss(x):\n",
    "    '''\n",
    "    Calculating the loss\n",
    "    input: the data in the form of \" movie : ( a list of user ids, an array of ratings )\"\n",
    "    output: the mean_squared_error\n",
    "    '''\n",
    "    n = 0\n",
    "    loss = 0\n",
    "\n",
    "    for m, (users, ratings) in x.items():\n",
    "        pred = W[users].dot(U[m]) + b[users] + c[m] + mu\n",
    "        loss += (ratings - pred).dot(ratings - pred)\n",
    "\n",
    "        # Count the total number of ratings\n",
    "        n += len(ratings)\n",
    "\n",
    "    return loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters to update\n",
    "k = 10                              # latent features\n",
    "\n",
    "W = np.random.randn(N, k)\n",
    "U = np.random.randn(M, k)\n",
    "\n",
    "b = np.zeros(N)\n",
    "c = np.zeros(M)\n",
    "\n",
    "mu = np.mean(list(um_to_rating_te.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch train loss : 0.7561272559634299 , test lss : 0.9145395731984773\n",
      "1th epoch train loss : 0.7561304501153409 , test lss : 0.9145366401795773\n",
      "2th epoch train loss : 0.7561233597067634 , test lss : 0.9145346220372841\n",
      "3th epoch train loss : 0.7561235640430944 , test lss : 0.9145323017916246\n",
      "4th epoch train loss : 0.7561229965832101 , test lss : 0.9145316255733037\n",
      "5th epoch train loss : 0.7561229945346621 , test lss : 0.9145307213245603\n",
      "6th epoch train loss : 0.7561229429573093 , test lss : 0.9145305367017733\n",
      "7th epoch train loss : 0.7561229375917137 , test lss : 0.9145302099297751\n",
      "8th epoch train loss : 0.7561229324692508 , test lss : 0.9145301557236964\n",
      "9th epoch train loss : 0.7561229302851759 , test lss : 0.9145300385521866\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 10\n",
    "reg = 0.1                # regularization penalty\n",
    "\n",
    "tr_losses = []\n",
    "te_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Gradient descent on W and b (the parameters related to the user a)\n",
    "    for a in range(N):\n",
    "        \n",
    "        # Now we can speed up the computation with the power of numpy\n",
    "        ###########################################\n",
    "        movies, rating_arr = user_to_mr[a]\n",
    "        term_1 = np.eye(k)*reg + np.dot(U[movies].T, U[movies])\n",
    "        term_2 = (rating_arr - b[a] - c[movies] - mu).dot(U[movies])\n",
    "        b_a = (rating_arr - U[movies].dot(W[a]) - c[movies] - mu).sum()\n",
    "        ###########################################\n",
    "\n",
    "        W[a] = np.linalg.solve(term_1, term_2)\n",
    "        b[a] = 1/(len(user_to_movie[a])*(1 + reg)) * b_a\n",
    "\n",
    "    # Gradient descent on U and c (the parameters related to the movie m)\n",
    "    for m in range(M):\n",
    "        try:\n",
    "            ###########################################\n",
    "            users, rating_arr = movie_to_ur[m]\n",
    "            term_1 = np.eye(k)*reg + np.dot(W[users].T, W[users])\n",
    "            term_2 = (rating_arr - b[users] - c[m] - mu).dot(W[users])\n",
    "            c_m = (rating_arr - W[users].dot(U[m]) - b[users] - mu).sum()\n",
    "            ###########################################\n",
    "        \n",
    "            U[b] = np.linalg.solve(term_1, term_2)\n",
    "            c[m] = 1/(len(movie_to_user[m])*(1+reg)) * c_m\n",
    "            \n",
    "        # for the case the movie m has no rating\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    tr_loss = get_loss(movie_to_ur_tr)\n",
    "    te_loss = get_loss(movie_to_ur_te)\n",
    "    print(\"{}th epoch train loss : {} , test lss : {}\".format(epoch, tr_loss, te_loss))\n",
    "\n",
    "    tr_losses.append(tr_loss)\n",
    "    te_losses.append(te_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train loss:  [0.7561272559634299, 0.7561304501153409, 0.7561233597067634, 0.7561235640430944, 0.7561229965832101, 0.7561229945346621, 0.7561229429573093, 0.7561229375917137, 0.7561229324692508, 0.7561229302851759]\n",
      "Total test loss:  [0.9145395731984773, 0.9145366401795773, 0.9145346220372841, 0.9145323017916246, 0.9145316255733037, 0.9145307213245603, 0.9145305367017733, 0.9145302099297751, 0.9145301557236964, 0.9145300385521866]\n"
     ]
    }
   ],
   "source": [
    "print(\"total train loss: \", tr_losses)\n",
    "print(\"Total test loss: \", te_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evaluation\n",
    "plt.plot(tr_losses, label = \"train loss\")\n",
    "plt.plot(te_losses, label = \"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
